{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rx8D_bIrVhCp",
        "outputId": "3f9baf22-567e-446d-ceba-b5eb5a72b916"
      },
      "outputs": [],
      "source": [
        "!pip install youtube-search-python yt-dlp\n",
        "!pip install --force-reinstall \"httpx<0.28\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "StPiNRjwVqgZ",
        "outputId": "732320b1-38a7-42bc-f154-9622b977a774"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import subprocess\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from youtubesearchpython import VideosSearch\n",
        "\n",
        "# Paths to your input and output CSV files\n",
        "input_csv = \"tracks_features.csv\"\n",
        "output_csv = \"new_songs_with_details.csv\"\n",
        "\n",
        "# Folder to store downloaded songs\n",
        "songs_folder = \"songs\"\n",
        "os.makedirs(songs_folder, exist_ok=True)\n",
        "\n",
        "# Additional columns to add\n",
        "ADDITIONAL_COLUMNS = [\"video_title\", \"video_url\", \"file_path\"]\n",
        "\n",
        "# ---------- Utility Functions ----------\n",
        "\n",
        "def clean_song_data(name, artist):\n",
        "    \"\"\"Strip and clean the 'name' and 'artist' fields.\"\"\"\n",
        "    clean_name = name.strip()\n",
        "    clean_artist = artist.strip()\n",
        "    if clean_artist.startswith(\"['\") and clean_artist.endswith(\"']\"):\n",
        "        clean_artist = clean_artist[2:-2].strip()\n",
        "    return clean_name, clean_artist\n",
        "\n",
        "def sanitize_filename(s):\n",
        "    \"\"\"Remove problematic characters and replace spaces with underscores.\"\"\"\n",
        "    s = s.strip()\n",
        "    s = re.sub(r'[^\\w\\s]', '', s)\n",
        "    s = s.replace(\" \", \"_\")\n",
        "    return s\n",
        "\n",
        "def initialize_output_csv(fieldnames):\n",
        "    \"\"\"Write header to the output CSV if it doesn't exist yet.\"\"\"\n",
        "    if not os.path.exists(output_csv):\n",
        "        with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "\n",
        "def get_processed_count():\n",
        "    \"\"\"Return how many rows have already been processed (based on output CSV).\"\"\"\n",
        "    if not os.path.exists(output_csv):\n",
        "        return 0\n",
        "    with open(output_csv, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile:\n",
        "        reader = csv.DictReader(infile)\n",
        "        return sum(1 for _ in reader)\n",
        "\n",
        "def append_row_to_csv(row, fieldnames):\n",
        "    \"\"\"Immediately append a single row to the output CSV.\"\"\"\n",
        "    # Create a new dict that only includes keys in 'fieldnames' (preserving order)\n",
        "    row_to_write = {col: row.get(col, \"\") for col in fieldnames}\n",
        "    try:\n",
        "        with open(output_csv, mode=\"a\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
        "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "            writer.writerow(row_to_write)\n",
        "    except Exception as write_error:\n",
        "        print(f\"Error writing row to CSV: {write_error}\")\n",
        "\n",
        "def process_row(row, fieldnames):\n",
        "    \"\"\"Download and process a single row, returning the updated row with new fields.\"\"\"\n",
        "    try:\n",
        "        # Check for name/artist values\n",
        "        name_raw = row.get(\"name\", \"\").strip()\n",
        "        artist_raw = row.get(\"artists\", \"\").strip()\n",
        "        if not name_raw or not artist_raw:\n",
        "            print(\"Skipping row due to missing data:\", row)\n",
        "            row[\"video_title\"] = \"\"\n",
        "            row[\"video_url\"] = \"\"\n",
        "            row[\"file_path\"] = \"\"\n",
        "            return row\n",
        "\n",
        "        # Clean the song name and artist\n",
        "        name, artist = clean_song_data(name_raw, artist_raw)\n",
        "        row[\"name\"] = name\n",
        "        row[\"artists\"] = artist\n",
        "\n",
        "        # Build the YouTube search query\n",
        "        query = f\"{name} by {artist}\"\n",
        "        print(f\"Searching for: {query}\")\n",
        "\n",
        "        # Perform YouTube search with error handling\n",
        "        try:\n",
        "            videosSearch = VideosSearch(query, limit=1)\n",
        "            result = videosSearch.result()\n",
        "        except Exception as e:\n",
        "            print(f\"Error searching for {query}: {e}\")\n",
        "            row[\"video_title\"] = \"\"\n",
        "            row[\"video_url\"] = \"\"\n",
        "            row[\"file_path\"] = \"\"\n",
        "            return row\n",
        "\n",
        "        if result.get(\"result\"):\n",
        "            video_info = result[\"result\"][0]\n",
        "            video_title = video_info.get(\"title\", \"\")\n",
        "            video_url = video_info.get(\"link\", \"\")\n",
        "            print(f\"Found video: {video_title} - {video_url}\")\n",
        "            row[\"video_title\"] = video_title\n",
        "            row[\"video_url\"] = video_url\n",
        "\n",
        "            # Build the output filename (MP3 format)\n",
        "            safe_song = sanitize_filename(name)\n",
        "            safe_artist = sanitize_filename(artist)\n",
        "            output_filename = os.path.join(songs_folder, f\"{safe_song}_{safe_artist}.mp3\")\n",
        "\n",
        "            # Download first 30 seconds of audio using yt-dlp\n",
        "            command = [\n",
        "                \"yt-dlp\",\n",
        "                \"--extract-audio\",\n",
        "                \"--audio-format\", \"mp3\",\n",
        "                \"--postprocessor-args\", \"ffmpeg:-ss 0\",\n",
        "                \"-o\", output_filename,\n",
        "                video_url\n",
        "            ]\n",
        "            print(f\"Downloading audio to {output_filename} ...\")\n",
        "            try:\n",
        "                subprocess.run(command, check=True)\n",
        "                row[\"file_path\"] = os.path.abspath(output_filename)\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"Error downloading {query}: {e}\")\n",
        "                if os.path.exists(output_filename):\n",
        "                    print(\"Audio file exists despite error; updating CSV accordingly.\")\n",
        "                    row[\"file_path\"] = os.path.abspath(output_filename)\n",
        "                else:\n",
        "                    row[\"file_path\"] = \"\"\n",
        "        else:\n",
        "            print(f\"No results found for {query}\")\n",
        "            row[\"video_title\"] = \"\"\n",
        "            row[\"video_url\"] = \"\"\n",
        "            row[\"file_path\"] = \"\"\n",
        "    except Exception as general_error:\n",
        "        print(f\"Unexpected error processing row {row}: {general_error}\")\n",
        "        row[\"video_title\"] = \"\"\n",
        "        row[\"video_url\"] = \"\"\n",
        "        row[\"file_path\"] = \"\"\n",
        "    return row\n",
        "\n",
        "# ---------- Main Script ----------\n",
        "\n",
        "sleep_interval = 5  # Time in seconds to wait before re-checking for new rows\n",
        "\n",
        "# Set the desired starting row (e.g., 1000 means start processing from the 1000th row)\n",
        "DESIRED_START_ROW = 100000\n",
        "\n",
        "# 1) Read the input CSV to get original fieldnames in order and load rows\n",
        "with open(input_csv, mode=\"r\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    original_fieldnames = reader.fieldnames or []  # The original column order from the input CSV\n",
        "    all_rows = list(reader)\n",
        "\n",
        "# 2) Create the final fieldnames list by appending new columns to the original order.\n",
        "fieldnames = list(original_fieldnames)\n",
        "for col in ADDITIONAL_COLUMNS:\n",
        "    if col not in fieldnames:\n",
        "        fieldnames.append(col)\n",
        "\n",
        "# 3) Initialize the output CSV (write header if it doesn't exist)\n",
        "initialize_output_csv(fieldnames)\n",
        "\n",
        "# 4) Determine how many rows have already been processed\n",
        "processed_count = get_processed_count()\n",
        "print(f\"Already processed {processed_count} rows.\")\n",
        "\n",
        "# 5) If processed_count is less than DESIRED_START_ROW, override it so that processing starts there.\n",
        "if processed_count < DESIRED_START_ROW:\n",
        "    print(f\"Setting starting row to {DESIRED_START_ROW}.\")\n",
        "    processed_count = DESIRED_START_ROW\n",
        "\n",
        "print(f\"Resuming from row {processed_count + 1} of the input CSV.\")\n",
        "\n",
        "print(\"Starting infinite processing loop...\")\n",
        "while True:\n",
        "    try:\n",
        "        with open(input_csv, mode=\"r\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            # Re-read original fieldnames in case the input CSV changes (optional)\n",
        "            original_fieldnames = reader.fieldnames or []\n",
        "            all_rows = list(reader)\n",
        "        total_rows = len(all_rows)\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading input CSV: {e}\")\n",
        "        total_rows = 0\n",
        "\n",
        "    if processed_count < total_rows:\n",
        "        # Process each new row one by one from the processed_count index onward\n",
        "        for row in all_rows[processed_count:]:\n",
        "            updated_row = process_row(row, fieldnames)\n",
        "            append_row_to_csv(updated_row, fieldnames)\n",
        "            processed_count += 1\n",
        "    else:\n",
        "        print(f\"No new rows (processed {processed_count} of {total_rows}). Waiting {sleep_interval} seconds...\")\n",
        "        time.sleep(sleep_interval)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
